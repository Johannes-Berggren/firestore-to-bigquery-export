# Firestore to BigQuery export
An automatic tool for copying and converting [Cloud Firestore](https://firebase.google.com/docs/firestore/) data to [BigQuery](https://cloud.google.com/bigquery/docs/).

<p align="center">
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/license-MIT-brightgreen.svg?" alt="Software License" />
  </a>
  <a href="https://npmjs.org/package/firestore-to-bigquery-export">
    <img src="https://img.shields.io/npm/v/firestore-to-bigquery-export.svg?" alt="Packagist" />
  </a>
  <a href="https://npmjs.org/package/firestore-to-bigquery-export">
    <img src="https://img.shields.io/npm/dm/firestore-to-bigquery-export.svg?" alt="Packagist" />
  </a>
  <a href="https://github.com/Johannes-Berggren/firestore-to-bigquery-export/issues">
    <img src="https://img.shields.io/github/issues/Johannes-Berggren/firestore-to-bigquery-export.svg?" alt="Issues" />
  </a>
</p>

Firestore is awesome. BigQuery is awesome. But transferring data from Firestore to BigQuery sucks.
This package lets you plug and play your way out of config hell.

- Create a BigQuery dataset with tables corresponding to your Firestore collections.
- Table schemas are automatically generated based on your document property data types.
- Convert and copy your Firestore collections to BigQuery.

This package doesn't write anything to Firestore.

## Contents
  * [Installation](#installation)
  * [How to](#how-to)
    + [API](#api)
    + [Examples](#examples)
  * [Limitations](#limitations)
  * [Issues](#issues)
  * [Issues](#to-do)

## Installation
> npm i firestore-to-bigquery-export

```javascript
import bigExport from 'firestore-to-bigquery-export'

// or

const bigExport = require('firestore-to-bigquery-export')

// then

const GCPSA = require('./Your-Service-Account-File.json')
bigExport.setBigQueryConfig(GCPSA)
bigExport.setFirebaseConfig(GCPSA)
```

## How to

### API
* `bigExport.setBigQueryConfig(serviceAccountFile:JSON)`
* `bigExport.setFirebaseConfig(serviceAccountFile:JSON)`
* `bigExport.createBigQueryTables(datasetID:string, collectionNames:Array):Promise<Array>`
* `bigExport.copyToBigQuery(datasetID:string, collectionName:string, snapshot:firebase.firestore.QuerySnapshot):Promise<number>`
* `bigExport.deleteBigQueryTables(datasetID:string, tableNames:Array):Promise<Array>`


### Examples
```javascript
/* Initialize BigQuery dataset named 'firestore' with four tables.
 * Table names equal collection names from Firestore.
 * Table schemas will be autogenerated based on the documents in the collections.
 */

bigExport.createBigQueryTables('firestore', [
        'payments',
        'profiles',
        'ratings',
        'users'
    ])
    .then(res => {
        console.log(res)
    })
    .catch(error => console.error(error))
```

Then, you can transport your data:
```javascript
/* Copying and converting all documents in the given QuerySnapshot.
 * Inserting each document as a row in tables with the same name as the collection, in the dataset named 'firestore'.
 * Cells (document properties) that doesn't match the table schema will be rejected.
 */
 
firebase.collection('payments').get()
    .then(snapshot => bigExport.copyToBigQuery('firestore', 'payments', snapshot))
    .then(res => {
        console.log('Copied ' + res + ' documents to BigQuery.')
    })
    .catch(error => console.error(error))

/*
 * You can do multiple collections async, like this.
 *
 * If you get error messages, you should probably copy fewer collections at a time.
 */

const collectionNames = ['payments', 'profiles', 'ratings', 'users']

Promise.all(collectionNames.map(n => {
    return firestore.collection(n).get()
      .then(c => bigExport.copyToBigQuery('firestore', n, c))
  }))
    .then(res => {
      console.log('Copied ' + res + ' documents to BigQuery.')
    })
    .catch(error => console.error(error))
```

After that, you may want to refresh your data. For the time being, the quick and dirty way is to delete your tables and make new ones:
```javascript
// Deleting the given BigQuery tables.

bigExport.deleteBigQueryTables('firestore', [
        'payments',
        'profiles',
        'ratings',
        'users'
    ])
    .then(res => {
        console.log(res)
    })
    .catch(error => console.error(error))
```

Maybe you want to refresh / overwrite a single table?
```javascript
// Overwrites the users BigQuery table with fresh data from the users collection in Firestore

bigExport.deleteBigQueryTables('firestore', 'users')
    .then(() => {
      return Promise.all([
        bigExport.createBigQueryTables('firestore', 'users'),
        firestore.collection('users').get()
      ])
    })
    .then(promises => {
      return bigExport.copyToBigQuery('firestore', 'users', promises[1])
    })
    .then(res => {
      console.log('Copied ' + res + ' documents to BigQuery.')
    })
    .catch(error => console.error(error))
```

## Limitations
* Your Firestore data model should be consistent. If a property of documents in the same collection have different data types, you'll get errors.
* Patching existing BigQuery sets isn't supported (yet). To refresh your datasets, you can `deleteBigQueryTables()`, then `createBigQueryTables()` and then `copyCollectionsToBigQuery()`.
* Changed your Firestore data model? Delete the corresponding BigQuery table and run `createBigQueryTables()` to create a table with a new schema.
* When running this package via a Cloud Function, you may experience that your function times out if your Firestore is large, (Deadline Exceeded). You can then:
    * Increase the timeout for your Cloud Function in the [Google Cloud Platform Cloud Function Console](https://console.cloud.google.com/functions).
    * Run your function locally, using `firebase serve --only functions`. 

## Issues
Please use the [issue tracker](https://github.com/Johannes-Berggren/firestore-to-bigquery-export/issues).

## To-do
* Implement continuous streaming of Firestore data to BigQuery.
* Implement patching of tables.
